{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62S_pk1osyRN"
      },
      "source": [
        "# **INF8953DE Reinforcement Learning (Fall 2021)**\n",
        "\n",
        "## **Final Project – Analysis of Deep Q-Network for Playing Atari**\n",
        "\n",
        "### **Authors:** Resende Silva, Luiz$^{1}$ & Talotsing, Gaëlle P. M.$^{2}$\n",
        "\n",
        "\\\n",
        "$1$ - luiz.resende-silva@polymtl.ca\n",
        "$2$ - gaelle-patricia-megouo.talotsing@polymtl.ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7fQzHLIs9gW"
      },
      "source": [
        "## **Python modules, classes and functions**\n",
        "\n",
        "### _MUST RUN_\n",
        "\n",
        "The cells below install the necessary Python modules and download and import environment ROMs and experiment scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dryvkoNa30"
      },
      "source": [
        "### *Installing Python modules*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhUSQOo6h8nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2c6839-115e-4dd9-9e9a-f429e4bea569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 8.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n",
            "Requirement already satisfied: torch==1.10.0+cu111 in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchaudio==0.10.0+cu111 in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: torchvision==0.11.1+cu111 in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting albumentations==1.1.0\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "     |████████████████████████████████| 102 kB 10.1 MB/s            \n",
            "\u001b[?25hCollecting ale-py==0.7.3\n",
            "  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 32.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: cmake==3.12.0 in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "Collecting datascience==0.17.0\n",
            "  Downloading datascience-0.17.0.tar.gz (721 kB)\n",
            "     |████████████████████████████████| 721 kB 73.4 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting folium==0.12.1.post1\n",
            "  Downloading folium-0.12.1.post1-py2.py3-none-any.whl (95 kB)\n",
            "     |████████████████████████████████| 95 kB 5.0 MB/s             \n",
            "\u001b[?25hCollecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "     |████████████████████████████████| 1.5 MB 97.2 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio-ffmpeg==0.4.5\n",
            "  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "     |████████████████████████████████| 26.9 MB 1.2 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pyglet==1.5.21\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 70.3 MB/s            \n",
            "\u001b[?25hCollecting pyvirtualdisplay==2.2\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Collecting opencv-python==4.5.4.60\n",
            "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "     |████████████████████████████████| 60.3 MB 124 kB/s             \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "     |████████████████████████████████| 18.3 MB 34.6 MB/s            \n",
            "\u001b[?25hCollecting wandb==0.12.7\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "     |████████████████████████████████| 1.7 MB 51.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick==1.3.post1 in /usr/local/lib/python3.7/dist-packages (1.3.post1)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (3.13)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "     |████████████████████████████████| 47.6 MB 1.1 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.3) (4.8.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.3) (5.4.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (1.8.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (57.4.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (5.5.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (4.4.1)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (0.4.2)\n",
            "Collecting nbsphinx\n",
            "  Downloading nbsphinx-0.8.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (3.6.4)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (3.7.1)\n",
            "Requirement already satisfied: coveralls in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (0.5)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from datascience==0.17.0) (2.3.3)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from folium==0.12.1.post1) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from folium==0.12.1.post1) (2.23.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "     |████████████████████████████████| 180 kB 61.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7) (7.1.2)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7) (3.17.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "     |████████████████████████████████| 140 kB 91.3 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "     |████████████████████████████████| 97 kB 9.1 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from yellowbrick==1.3.post1) (1.0.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "     |████████████████████████████████| 63 kB 2.3 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb==0.12.7) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py==0.7.3) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.9->folium==0.12.1.post1) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->folium==0.12.1.post1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->folium==0.12.1.post1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->folium==0.12.1.post1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->folium==0.12.1.post1) (3.0.4)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->yellowbrick==1.3.post1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->yellowbrick==1.3.post1) (3.0.0)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb==0.12.7) (1.1.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->datascience==0.17.0) (5.1.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->datascience==0.17.0) (21.3)\n",
            "Requirement already satisfied: docopt>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from coveralls->datascience==0.17.0) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->datascience==0.17.0) (0.8.1)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.7/dist-packages (from nbsphinx->datascience==0.17.0) (0.17.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from nbsphinx->datascience==0.17.0) (5.1.3)\n",
            "Requirement already satisfied: nbconvert!=5.4 in /usr/local/lib/python3.7/dist-packages (from nbsphinx->datascience==0.17.0) (5.6.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->datascience==0.17.0) (2.9.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->datascience==0.17.0) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->datascience==0.17.0) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->datascience==0.17.0) (2.2.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->datascience==0.17.0) (1.3.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->datascience==0.17.0) (1.3.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->datascience==0.17.0) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->datascience==0.17.0) (8.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->datascience==0.17.0) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->datascience==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->datascience==0.17.0) (1.11.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (4.9.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert!=5.4->nbsphinx->datascience==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat->nbsphinx->datascience==0.17.0) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->nbsphinx->datascience==0.17.0) (2.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->datascience==0.17.0) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->datascience==0.17.0) (0.7.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->datascience==0.17.0) (1.1.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert!=5.4->nbsphinx->datascience==0.17.0) (0.5.1)\n",
            "Building wheels for collected packages: datascience, gym, subprocess32, pathtools\n",
            "  Building wheel for datascience (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datascience: filename=datascience-0.17.0-py3-none-any.whl size=726848 sha256=e59b15865a957e512fe606ba6b1d7618dc7d8698aacb2689bd636a5f1a855f53\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/e0/1b/bc890a517ad4212eb7180ccca6497c97d0ca7c28342b236888\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=29438221c5ac8ed64b1d2e344584b32642bc7704b329819a86b6fb17886e1453\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=20980365cb87b9abc73849014b4752022004c5b58e551df84f05936d190d39e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=ce502129911afd086287ee26988055eb47e30fa0d0bb883e40ab2800cf71a335\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built datascience gym subprocess32 pathtools\n",
            "Installing collected packages: smmap, opencv-python-headless, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, qudida, pathtools, nbsphinx, GitPython, folium, EasyProcess, docker-pycreds, configparser, wandb, pyvirtualdisplay, pyglet, pygame, opencv-python, imageio-ffmpeg, gym, datascience, ale-py, albumentations\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "  Attempting uninstall: datascience\n",
            "    Found existing installation: datascience 0.10.6\n",
            "    Uninstalling datascience-0.10.6:\n",
            "      Successfully uninstalled datascience-0.10.6\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed EasyProcess-0.3 GitPython-3.1.24 albumentations-1.1.0 ale-py-0.7.3 configparser-5.2.0 datascience-0.17.0 docker-pycreds-0.4.0 folium-0.12.1.post1 gitdb-4.0.9 gym-0.21.0 imageio-ffmpeg-0.4.5 nbsphinx-0.8.7 opencv-python-4.5.4.60 opencv-python-headless-4.5.4.60 pathtools-0.1.2 pygame-2.1.0 pyglet-1.5.21 pyvirtualdisplay-2.2 qudida-0.0.4 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (635 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install torch==1.10.0+cu111 torchaudio==0.10.0+cu111 torchvision==0.11.1+cu111\n",
        "!python -m pip install albumentations==1.1.0 ale-py==0.7.3 cmake==3.12.0 datascience==0.17.0 folium==0.12.1.post1 gym==0.21.0 imageio-ffmpeg==0.4.5 matplotlib==3.2.2 numpy==1.19.5 pandas==1.1.5 pyglet==1.5.21 pyvirtualdisplay==2.2 opencv-python==4.5.4.60  pygame==2.1.0 wandb==0.12.7 yellowbrick==1.3.post1\n",
        "!sudo apt-get install xvfb  # COMMENT-OUT IF RUNNING ON WINDOWS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx5DgyW-NlZH"
      },
      "source": [
        "### *Weights & Biases*\n",
        "\n",
        "The experiments set up uses [Weights & Biases](https://wandb.ai/site) to log training information. The ``wandb`` module allows for code automation, real-time visualization of the training process and general code cleanness.\n",
        "\n",
        "To use Weights & Biases, the user must create an account, go to the Weights & Biases account settings, retrieve the personal API key and replace ``<key>`` with the personal API key number (40-character combination). However, if the user does not want to use the ``wandb`` module, skip this cell and ensure that ``'use_wandb_logging': True`` inside the dictionary ``config_training``. The object of class ``AgentDQN`` will still have the arguments ``episodes_scores`` and ``episodes_losses`` that can be used as training quality metrics.\n",
        "\n",
        "**NOTE FOR WINDOWS**: remove the \"!\" if running on Windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I29D_ZQQCzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5887f5-6b6b-4074-f80d-c8a5f7f95ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "# LOGIN TO WEIGHTS & BIASES LOGGING MODULE\n",
        "!wandb login de27dda9fe6edcfd2dd6e97800a893ece3e1958e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz5pXpPXORIC"
      },
      "source": [
        "### *Atari Environment ROMS*\n",
        "\n",
        "Downloading and installing the Atari ROMS for the [Arcade Learning Environment (ALE)](https://github.com/mgbellemare/Arcade-Learning-Environment), as well as the [MinAtar](https://github.com/kenjyoung/MinAtar) testbed and the modified ``env/registration.py`` script for [OpenAI Gym](https://gym.openai.com/docs/) (allows creating MinAtar-Gym environment).\n",
        "\n",
        "**NOTE FOR WINDOWS**: running this Python notebook on Windows, the commands ``!wget`` and ``!unzip`` will not be supported. The user should download and extract files in the below links and change the directory paths to import the Atari ROMS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XGNy9Tol0yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4086a308-257b-4e8c-b0c7-e9486efe6a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 04:04:18--  https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/ROMS.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/ROMS.zip [following]\n",
            "--2021-12-14 04:04:18--  https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/ROMS.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 421859 (412K) [application/zip]\n",
            "Saving to: ‘ROMS.zip’\n",
            "\n",
            "ROMS.zip            100%[===================>] 411.97K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-12-14 04:04:18 (14.7 MB/s) - ‘ROMS.zip’ saved [421859/421859]\n",
            "\n",
            "--2021-12-14 04:04:18--  https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/MinAtar_gym.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/MinAtar_gym.zip [following]\n",
            "--2021-12-14 04:04:19--  https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/MinAtar_gym.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3088 (3.0K) [application/zip]\n",
            "Saving to: ‘MinAtar_gym.zip’\n",
            "\n",
            "MinAtar_gym.zip     100%[===================>]   3.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-14 04:04:19 (55.1 MB/s) - ‘MinAtar_gym.zip’ saved [3088/3088]\n",
            "\n",
            "--2021-12-14 04:04:19--  https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/MinAtarModule.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/MinAtarModule.zip [following]\n",
            "--2021-12-14 04:04:19--  https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/MinAtarModule.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58813 (57K) [application/zip]\n",
            "Saving to: ‘MinAtarModule.zip’\n",
            "\n",
            "MinAtarModule.zip   100%[===================>]  57.43K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-12-14 04:04:19 (10.4 MB/s) - ‘MinAtarModule.zip’ saved [58813/58813]\n",
            "\n",
            "Archive:  ./ROMS.zip\n",
            "   creating: ROMS/\n",
            "  inflating: ROMS/adventure.bin      \n",
            "  inflating: ROMS/air_raid.bin       \n",
            "  inflating: ROMS/alien.bin          \n",
            "  inflating: ROMS/amidar.bin         \n",
            "  inflating: ROMS/assault.bin        \n",
            "  inflating: ROMS/asterix.bin        \n",
            "  inflating: ROMS/asteroids.bin      \n",
            "  inflating: ROMS/atlantis.bin       \n",
            "  inflating: ROMS/atlantis2.bin      \n",
            "  inflating: ROMS/backgammon.bin     \n",
            "  inflating: ROMS/bank_heist.bin     \n",
            "  inflating: ROMS/basic_math.bin     \n",
            "  inflating: ROMS/battle_zone.bin    \n",
            "  inflating: ROMS/beam_rider.bin     \n",
            "  inflating: ROMS/berzerk.bin        \n",
            "  inflating: ROMS/blackjack.bin      \n",
            "  inflating: ROMS/bowling.bin        \n",
            "  inflating: ROMS/boxing.bin         \n",
            "  inflating: ROMS/breakout.bin       \n",
            "  inflating: ROMS/carnival.bin       \n",
            "  inflating: ROMS/casino.bin         \n",
            "  inflating: ROMS/centipede.bin      \n",
            "  inflating: ROMS/chopper_command.bin  \n",
            "  inflating: ROMS/crazy_climber.bin  \n",
            "  inflating: ROMS/crossbow.bin       \n",
            "  inflating: ROMS/darkchambers.bin   \n",
            "  inflating: ROMS/defender.bin       \n",
            "  inflating: ROMS/demon_attack.bin   \n",
            "  inflating: ROMS/donkey_kong.bin    \n",
            "  inflating: ROMS/double_dunk.bin    \n",
            "  inflating: ROMS/earthworld.bin     \n",
            "  inflating: ROMS/elevator_action.bin  \n",
            "  inflating: ROMS/enduro.bin         \n",
            "  inflating: ROMS/entombed.bin       \n",
            "  inflating: ROMS/et.bin             \n",
            "  inflating: ROMS/fishing_derby.bin  \n",
            "  inflating: ROMS/flag_capture.bin   \n",
            "  inflating: ROMS/freeway.bin        \n",
            "  inflating: ROMS/frogger.bin        \n",
            "  inflating: ROMS/frostbite.bin      \n",
            "  inflating: ROMS/galaxian.bin       \n",
            "  inflating: ROMS/gopher.bin         \n",
            "  inflating: ROMS/gravitar.bin       \n",
            "  inflating: ROMS/hangman.bin        \n",
            "  inflating: ROMS/haunted_house.bin  \n",
            "  inflating: ROMS/hero.bin           \n",
            "  inflating: ROMS/human_cannonball.bin  \n",
            "  inflating: ROMS/ice_hockey.bin     \n",
            "  inflating: ROMS/jamesbond.bin      \n",
            "  inflating: ROMS/journey_escape.bin  \n",
            "  inflating: ROMS/kaboom.bin         \n",
            "  inflating: ROMS/kangaroo.bin       \n",
            "  inflating: ROMS/keystone_kapers.bin  \n",
            "  inflating: ROMS/king_kong.bin      \n",
            "  inflating: ROMS/klax.bin           \n",
            "  inflating: ROMS/koolaid.bin        \n",
            "  inflating: ROMS/krull.bin          \n",
            "  inflating: ROMS/kung_fu_master.bin  \n",
            "  inflating: ROMS/laser_gates.bin    \n",
            "  inflating: ROMS/lost_luggage.bin   \n",
            "  inflating: ROMS/mario_bros.bin     \n",
            "  inflating: ROMS/miniature_golf.bin  \n",
            "  inflating: ROMS/montezuma_revenge.bin  \n",
            "  inflating: ROMS/mr_do.bin          \n",
            "  inflating: ROMS/ms_pacman.bin      \n",
            "  inflating: ROMS/name_this_game.bin  \n",
            "  inflating: ROMS/othello.bin        \n",
            "  inflating: ROMS/pacman.bin         \n",
            "  inflating: ROMS/phoenix.bin        \n",
            "  inflating: ROMS/pitfall.bin        \n",
            "  inflating: ROMS/pitfall2.bin       \n",
            "  inflating: ROMS/pong.bin           \n",
            "  inflating: ROMS/pooyan.bin         \n",
            "  inflating: ROMS/private_eye.bin    \n",
            "  inflating: ROMS/qbert.bin          \n",
            "  inflating: ROMS/riverraid.bin      \n",
            "  inflating: ROMS/road_runner.bin    \n",
            "  inflating: ROMS/robotank.bin       \n",
            "  inflating: ROMS/seaquest.bin       \n",
            "  inflating: ROMS/sir_lancelot.bin   \n",
            "  inflating: ROMS/skiing.bin         \n",
            "  inflating: ROMS/solaris.bin        \n",
            "  inflating: ROMS/space_invaders.bin  \n",
            "  inflating: ROMS/space_war.bin      \n",
            "  inflating: ROMS/star_gunner.bin    \n",
            "  inflating: ROMS/superman.bin       \n",
            "  inflating: ROMS/surround.bin       \n",
            "  inflating: ROMS/tennis.bin         \n",
            "  inflating: ROMS/tetris.bin         \n",
            "  inflating: ROMS/tic_tac_toe_3d.bin  \n",
            "  inflating: ROMS/time_pilot.bin     \n",
            "  inflating: ROMS/trondead.bin       \n",
            "  inflating: ROMS/turmoil.bin        \n",
            "  inflating: ROMS/tutankham.bin      \n",
            "  inflating: ROMS/up_n_down.bin      \n",
            "  inflating: ROMS/venture.bin        \n",
            "  inflating: ROMS/videochess.bin     \n",
            "  inflating: ROMS/videocube.bin      \n",
            "  inflating: ROMS/video_checkers.bin  \n",
            "  inflating: ROMS/video_pinball.bin  \n",
            "  inflating: ROMS/wizard_of_wor.bin  \n",
            "  inflating: ROMS/word_zapper.bin    \n",
            "  inflating: ROMS/yars_revenge.bin   \n",
            "  inflating: ROMS/zaxxon.bin         \n",
            "Archive:  ./MinAtarModule.zip\n",
            "   creating: MinAtar/\n",
            "  inflating: MinAtar/.gitignore      \n",
            "   creating: MinAtar/examples/\n",
            "  inflating: MinAtar/examples/AC_lambda.py  \n",
            "  inflating: MinAtar/examples/agent_play.py  \n",
            "  inflating: MinAtar/examples/dqn.py  \n",
            "  inflating: MinAtar/examples/human_play.py  \n",
            "  inflating: MinAtar/examples/plot_return.py  \n",
            "  inflating: MinAtar/examples/random_play.py  \n",
            "  inflating: MinAtar/License.txt     \n",
            "   creating: MinAtar/minatar/\n",
            "  inflating: MinAtar/minatar/environment.py  \n",
            "   creating: MinAtar/minatar/environments/\n",
            "  inflating: MinAtar/minatar/environments/asterix.py  \n",
            "  inflating: MinAtar/minatar/environments/breakout.py  \n",
            "  inflating: MinAtar/minatar/environments/freeway.py  \n",
            "  inflating: MinAtar/minatar/environments/seaquest.py  \n",
            "  inflating: MinAtar/minatar/environments/space_invaders.py  \n",
            "  inflating: MinAtar/minatar/environments/__init__.py  \n",
            "   creating: MinAtar/minatar/environments/__pycache__/\n",
            "  inflating: MinAtar/minatar/environments/__pycache__/breakout.cpython-37.pyc  \n",
            "  inflating: MinAtar/minatar/environments/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: MinAtar/minatar/gui.py  \n",
            "  inflating: MinAtar/minatar/gym.py  \n",
            "  inflating: MinAtar/minatar/__init__.py  \n",
            "   creating: MinAtar/minatar/__pycache__/\n",
            "  inflating: MinAtar/minatar/__pycache__/environment.cpython-37.pyc  \n",
            "  inflating: MinAtar/minatar/__pycache__/gui.cpython-37.pyc  \n",
            "  inflating: MinAtar/minatar/__pycache__/__init__.cpython-37.pyc  \n",
            "   creating: MinAtar/MinAtar.egg-info/\n",
            "  inflating: MinAtar/MinAtar.egg-info/dependency_links.txt  \n",
            "  inflating: MinAtar/MinAtar.egg-info/entry_points.txt  \n",
            "  inflating: MinAtar/MinAtar.egg-info/PKG-INFO  \n",
            "  inflating: MinAtar/MinAtar.egg-info/requires.txt  \n",
            "  inflating: MinAtar/MinAtar.egg-info/SOURCES.txt  \n",
            "  inflating: MinAtar/MinAtar.egg-info/top_level.txt  \n",
            "  inflating: MinAtar/README.md       \n",
            "  inflating: MinAtar/setup.py        \n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       journey_escape        ROMS/journey_escape.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               enduro                ROMS/enduro.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                krull                 ROMS/krull.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            asteroids             ROMS/asteroids.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           backgammon            ROMS/backgammon.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong           ROMS/donkey_kong.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround              ROMS/surround.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tetris                ROMS/tetris.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          star_gunner           ROMS/star_gunner.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              phoenix               ROMS/phoenix.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         sir_lancelot          ROMS/sir_lancelot.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pooyan                ROMS/pooyan.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        wizard_of_wor         ROMS/wizard_of_wor.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               amidar                ROMS/amidar.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         flag_capture          ROMS/flag_capture.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          laser_gates           ROMS/laser_gates.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            up_n_down             ROMS/up_n_down.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             crossbow              ROMS/crossbow.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders        ROMS/space_invaders.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m    montezuma_revenge     ROMS/montezuma_revenge.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           time_pilot            ROMS/time_pilot.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      chopper_command       ROMS/chopper_command.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 hero                  ROMS/hero.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       video_checkers        ROMS/video_checkers.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             air_raid              ROMS/air_raid.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               kaboom                ROMS/kaboom.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           videochess            ROMS/videochess.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         lost_luggage          ROMS/lost_luggage.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                mr_do                 ROMS/mr_do.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            space_war             ROMS/space_war.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              pitfall               ROMS/pitfall.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              bowling               ROMS/bowling.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         demon_attack          ROMS/demon_attack.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               casino                ROMS/casino.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            centipede             ROMS/centipede.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              frogger               ROMS/frogger.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       name_this_game        ROMS/name_this_game.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            king_kong             ROMS/king_kong.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                   et                    ROMS/et.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         yars_revenge          ROMS/yars_revenge.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             superman              ROMS/superman.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          road_runner           ROMS/road_runner.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              koolaid               ROMS/koolaid.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            jamesbond             ROMS/jamesbond.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           bank_heist            ROMS/bank_heist.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          double_dunk           ROMS/double_dunk.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             defender              ROMS/defender.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            blackjack             ROMS/blackjack.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          word_zapper           ROMS/word_zapper.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            ms_pacman             ROMS/ms_pacman.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pacman                ROMS/pacman.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           mario_bros            ROMS/mario_bros.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        crazy_climber         ROMS/crazy_climber.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math            ROMS/basic_math.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       tic_tac_toe_3d        ROMS/tic_tac_toe_3d.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              turmoil               ROMS/turmoil.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             carnival              ROMS/carnival.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              assault               ROMS/assault.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             robotank              ROMS/robotank.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             entombed              ROMS/entombed.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            videocube             ROMS/videocube.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 pong                  ROMS/pong.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           earthworld            ROMS/earthworld.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               skiing                ROMS/skiing.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master        ROMS/kung_fu_master.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             pitfall2              ROMS/pitfall2.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               gopher                ROMS/gopher.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             gravitar              ROMS/gravitar.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              venture               ROMS/venture.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              othello               ROMS/othello.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            riverraid             ROMS/riverraid.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        haunted_house         ROMS/haunted_house.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix               ROMS/asterix.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m     human_cannonball      ROMS/human_cannonball.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              hangman               ROMS/hangman.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            adventure             ROMS/adventure.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               zaxxon                ROMS/zaxxon.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               boxing                ROMS/boxing.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout              ROMS/breakout.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             kangaroo              ROMS/kangaroo.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        fishing_derby         ROMS/fishing_derby.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           ice_hockey            ROMS/ice_hockey.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             galaxian              ROMS/galaxian.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            tutankham             ROMS/tutankham.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball         ROMS/video_pinball.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      elevator_action       ROMS/elevator_action.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tennis                ROMS/tennis.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              freeway               ROMS/freeway.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          private_eye           ROMS/private_eye.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         darkchambers          ROMS/darkchambers.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          battle_zone           ROMS/battle_zone.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       miniature_golf        ROMS/miniature_golf.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert                 ROMS/qbert.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            atlantis2             ROMS/atlantis2.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              berzerk               ROMS/berzerk.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             seaquest              ROMS/seaquest.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             trondead              ROMS/trondead.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              solaris               ROMS/solaris.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 klax                  ROMS/klax.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            frostbite             ROMS/frostbite.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                alien                 ROMS/alien.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             atlantis              ROMS/atlantis.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      keystone_kapers       ROMS/keystone_kapers.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           beam_rider            ROMS/beam_rider.bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 104 / 104 ROMs\n",
            "Processing ./MinAtar\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (2018.9)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.4.1)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (0.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar==1.0.10) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->MinAtar==1.0.10) (3.10.0.2)\n",
            "Building wheels for collected packages: MinAtar\n",
            "  Building wheel for MinAtar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MinAtar: filename=MinAtar-1.0.10-py3-none-any.whl size=16281 sha256=a399c43b3f967283fb7b8e558c2911f56ca3ad69084ba583cd4c0fbca98d1c69\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1vawnixo/wheels/22/45/e7/9485b3b3df1b5b530540ee024d6f7a31d29be0b41a2d625d33\n",
            "Successfully built MinAtar\n",
            "Installing collected packages: MinAtar\n",
            "Successfully installed MinAtar-1.0.10\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# RETRIEVING ATARI ROMS, MINATAR MODULE AND GYM ENVIRONMENT REGISTRATION ALLOWING MINATAR\n",
        "!wget https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/ROMS.zip\n",
        "!wget https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/MinAtar_gym.zip\n",
        "!wget https://github.com/luiz-resende/ReinforcementLearning/raw/main/A03_DQN/MinAtarModule.zip\n",
        "!unzip './ROMS.zip'\n",
        "!unzip './MinAtarModule.zip'\n",
        "!ale-import-roms './ROMS'\n",
        "\n",
        "!python -m pip install ./MinAtar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vEURfE6DG2w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "\n",
        "zipfile.ZipFile('MinAtar_gym.zip', 'r').extractall((os.path.abspath(np.__file__)[:-17] + 'gym/envs/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fe2XHN4QTDh"
      },
      "source": [
        "### *Agent, Model, Memory Buffer and Environment Wrapper scripts*\n",
        "\n",
        "[Luiz Resende Silva](mailto:luiz.resende-silva@polymtl.ca) implemented the scripts for the different classes used in the experiment, keeping them in his [GitHub page](https://github.com/luiz-resende/ReinforcementLearning/tree/main/A03_DQN) to maintain a clean and small-sized ``.ipynb`` file and also make centralized editions to the code. The scripts are downloaded and imported into this python notebook using the code in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIWIDctWtYnb"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "def import_github_script(URL: str, file_name: str) -> None:\n",
        "    r\"\"\"\n",
        "    Function retrieves Python scripts from github page and creates an importable file with extension ``.py``.\n",
        "\n",
        "    This function aims to allow the code modifications to be made at Github, maintaining cleaness of coding\n",
        "    and allowing modifications to be readily available in any copy of the notebook.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    URL : ``str``\n",
        "        URL address containing the file.\n",
        "    file_name : ``str``\n",
        "        Name to be given to the downloaded file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ``None``\n",
        "    \"\"\"\n",
        "    path = file_name + '.py'\n",
        "    if (not os.path.isfile(path)):\n",
        "        raw_bytes = urllib.request.urlopen(URL).read()\n",
        "        raw_str = raw_bytes.decode(\"utf-8\")\n",
        "        mode = 'x'\n",
        "        text_file = open(path, mode)\n",
        "        text_file.write(raw_str)\n",
        "        text_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2eUtbxVmOm4"
      },
      "outputs": [],
      "source": [
        "# DOWNLOADING SCRIPTS FROM GITHUB\n",
        "import_github_script(r'https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/utils/dqn_memory_buffer.py',\n",
        "                     r'dqn_memory_buffer')\n",
        "import_github_script(r'https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/models/dqn_models_torch.py',\n",
        "                     r'dqn_models_torch')\n",
        "import_github_script(r'https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/utils/dqn_wrappers_env.py',\n",
        "                     r'dqn_wrappers_env')\n",
        "import_github_script(r'https://raw.githubusercontent.com/luiz-resende/ReinforcementLearning/main/A03_DQN/agent/dqn_agent_atari.py',\n",
        "                     r'dqn_agent_atari')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyilXg4JGOu"
      },
      "source": [
        "## **Training Pipeline**\n",
        "\n",
        "The different experiments performed are shown and commented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue3xD_8VwSQk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from dqn_agent_atari import AgentDQN  # IMPORTING EXPERIMENT AGENT SCRIPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "focUg0hdcC13"
      },
      "source": [
        "### 1 - Testing size of experience replay memory\n",
        "\n",
        "All experiments in this section used the hyper-parameters shown in the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW8dwU9NcSL4"
      },
      "outputs": [],
      "source": [
        "config_env = {'game_id': 'Pong-v4',\n",
        "              'is_minatar': False,\n",
        "              'render_mode': 'rgb_array',\n",
        "              'max_episode_steps': None,\n",
        "              'no_op_reset_env': True,\n",
        "              'no_op_max_env': 30,\n",
        "              'skip_frames_env': True,\n",
        "              'skip_frames_env_n': 4,\n",
        "              'wrap_env': True,\n",
        "              'clip_rewards': True,\n",
        "              'episodic_life': True,\n",
        "              'scale_frame': False,\n",
        "              'stack_frames': True,\n",
        "              'warp_frames': True,\n",
        "              'warp_frames_greyscale': True,\n",
        "              }\n",
        "\n",
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 2,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "config_optim = {'optimizer': 'RMSprop',\n",
        "                'loss_criterion': 'smooth_l1_loss',  # 'huber_loss'\n",
        "                'gamma_disc': 0.99,\n",
        "                'learn_rate': 0.00025,\n",
        "                'grad_momentum': 0.95,\n",
        "                'grad_momentum_square': 0.95,\n",
        "                'min_sqr_grad': 0.01,\n",
        "                'epsilon_max': 1.00,\n",
        "                'epsilon_min': 0.10,\n",
        "                'eps_decay_interval': 1000000,\n",
        "                'exponential_decay': False,\n",
        "                'target_network_update': 10000,\n",
        "                }\n",
        "\n",
        "config_training = {'seed': 895359,\n",
        "                   'use_wandb_logging': True,\n",
        "                   'experiment_project_name': 'RL_ProjectTest_-_DQN_Atari_Pong-v4'\n",
        "                   'train_in_episodes': False,\n",
        "                   'max_number_training_frames': 5000000,\n",
        "                   }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KXnU1MMcdVN"
      },
      "source": [
        "#### 1.1 - Experience replay memory capacity equal to $100$k samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXPzJJYOiT6e"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 100000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                  }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_ReplayMemory_size=100k',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frame and 100k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnQFmmMqiT6l",
        "outputId": "850f3db2-0bcb-4636-a111-c15191ee3a0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [7:23:53<00:00, 187.73 frames/s, AvgRewardEps=16.7, AvgSteps=1.98e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvRsyNtpiT6m",
        "outputId": "62076d5d-3dba-47c8-8d5d-2e57eb4d705e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.18 +/- 4.048\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Z6jSQWdNiF"
      },
      "source": [
        "#### 1.2 - Experience replay memory capacity equal to $250$k samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlpnM_YQdNiQ"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 250000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                  }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_ReplayMemory_size=250k',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frame and 250k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdJzrMzUdNiR",
        "outputId": "77139d10-8fd8-444b-b332-05dcd8e015c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [7:23:28<00:00, 187.91 frames/s, AvgRewardEps=16, AvgSteps=1.98e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOpcm_CIeYc5",
        "outputId": "cc2a6bb1-ac35-48a5-f0f8-f5683d2a45fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.74 +/- 1.82\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tNCS7uhBfx"
      },
      "source": [
        "#### 1.3 - Experience replay memory capacity equal to $500$k samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlYZGIQ7hBf6"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                  }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_ReplayMemory_size=500k',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frame and 500k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h8wEASkhBf6",
        "outputId": "1abdadae-2de8-4b2c-ba70-dd3308bcf937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [9:33:06<00:00, 145.40 frames/s, AvgRewardEps=16.2, AvgSteps=1983.33, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjarJ0_6hBf7",
        "outputId": "469f70f4-fbcf-43c4-dc52-2d3fc8b95602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 21.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXlgGlchOp1"
      },
      "source": [
        "#### 1.4 - Experience replay memory capacity equal to $1$M samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLBFR5dYhOp1"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 1000000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                  }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_ReplayMemory_size=1M',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frame and 1M memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kinFp2zhOp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfdc4d1-d680-40ec-ff73-ad4f465d173e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [41:42:50<00:00, 33.30 frames/s, AvgRewardEps=15.3, AvgSteps=2.02e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gd4oUgzhOp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6820684f-fbc3-48ed-ca99-d0737c2c2e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.52 +/- 0.608\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTKueUg8lWOI"
      },
      "source": [
        "### 2 - Testing policy model update frequency\n",
        "\n",
        "All experiments in this section used the hyper-parameters shown in the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8yFLMiRlWOL"
      },
      "outputs": [],
      "source": [
        "config_env = {'game_id': 'Pong-v4',\n",
        "              'is_minatar': False,\n",
        "              'render_mode': 'rgb_array',\n",
        "              'max_episode_steps': None,\n",
        "              'no_op_reset_env': True,\n",
        "              'no_op_max_env': 30,\n",
        "              'skip_frames_env': True,\n",
        "              'skip_frames_env_n': 4,\n",
        "              'wrap_env': True,\n",
        "              'clip_rewards': True,\n",
        "              'episodic_life': True,\n",
        "              'scale_frame': False,\n",
        "              'stack_frames': True,\n",
        "              'warp_frames': True,\n",
        "              'warp_frames_greyscale': True,\n",
        "              }\n",
        "\n",
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 2,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "config_optim = {'optimizer': 'RMSprop',\n",
        "                'loss_criterion': 'smooth_l1_loss',  # 'huber_loss'\n",
        "                'gamma_disc': 0.99,\n",
        "                'learn_rate': 0.00025,\n",
        "                'grad_momentum': 0.95,\n",
        "                'grad_momentum_square': 0.95,\n",
        "                'min_sqr_grad': 0.01,\n",
        "                'epsilon_max': 1.00,\n",
        "                'epsilon_min': 0.10,\n",
        "                'eps_decay_interval': 1000000,\n",
        "                'exponential_decay': False,\n",
        "                'target_network_update': 10000,\n",
        "                }\n",
        "\n",
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                  }\n",
        "\n",
        "config_training = {'seed': 895359,\n",
        "                   'use_wandb_logging': True,\n",
        "                   'experiment_project_name': 'RL_ProjectTest_-_DQN_Atari_Pong-v4',\n",
        "                   'train_in_episodes': False,\n",
        "                   'max_number_training_frames': 5000000,\n",
        "                   }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCbZkygA5pFN"
      },
      "source": [
        "#### 2.1 - Update policy model after every $k=1$ steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oppiza4WDy_c"
      },
      "outputs": [],
      "source": [
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_UpdateFrequency_k=1',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 1 frame and 500k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2WlDBSYlWOM",
        "outputId": "2b95affa-671f-4986-f46a-a2c5c13944f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [32:12:39<00:00, 43.12 frames/s, AvgRewardEps=14.7, AvgSteps=1.97e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=1,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qo2WqLcrjP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cab530-80e8-445b-f1f7-44569b6fa2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.68 +/- 0.466\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txqfo-dflIzi"
      },
      "source": [
        "#### 2.2 - Update policy model after every $k=4$ steps (base-line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbvuJb8YDvY8"
      },
      "outputs": [],
      "source": [
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_UpdateFrequency_k=4_(baseline)',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SOph0sTpbBY",
        "outputId": "f0a51737-5330-44eb-8b2a-04ea7d2a48d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [9:33:06<00:00, 145.40 frames/s, AvgRewardEps=16.2, AvgSteps=1983.33, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k30nB9pmlE5O",
        "outputId": "0158db8b-8474-4c6a-9e61-eb434ba10019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 21.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHTtVsAUHUgj"
      },
      "source": [
        "#### 2.3 - Update policy model after every $k=5$ steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WaNoCXzHUgp"
      },
      "outputs": [],
      "source": [
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_UpdateFrequency_k=5',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 5 frames and 500k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d36f19-5bdc-4fa0-eb48-6f766b553773",
        "id": "35WfsZlaHUgp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [6:53:07<00:00, 201.71 frames/s, AvgRewardEps=16.1, AvgSteps=1.98e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=5,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54e481b-6568-47a5-a98a-b94dee122c9f",
        "id": "6xwJb0juHUgp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.96 +/- 0.28\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()  # Evaluation time 3.9 min\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd0MUPhSl3cQ"
      },
      "source": [
        "#### 2.4 - Update policy model after every $k=10$ steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB3zJv3GCXJo"
      },
      "outputs": [],
      "source": [
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_UpdateFrequency_k=10',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 10 frames and 500k memory capacity.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmMMzKXKl3cT",
        "outputId": "a96cb545-a594-4578-c334-60b220bf02c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [5:37:36<00:00, 246.83 frames/s, AvgRewardEps=16.3, AvgSteps=2.04e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=10,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOiHj-0glJNJ",
        "outputId": "4d80a3ea-0ea8-4015-a570-81b573e8524e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.82 +/- 0.384\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Nyj06hqhsE"
      },
      "source": [
        "### 3 - Testing different network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVq7v3ufleUZ"
      },
      "outputs": [],
      "source": [
        "config_env = {'game_id': 'Pong-v4',\n",
        "              'is_minatar': False,\n",
        "              'render_mode': 'rgb_array',\n",
        "              'max_episode_steps': None,\n",
        "              'no_op_reset_env': True,\n",
        "              'no_op_max_env': 30,\n",
        "              'skip_frames_env': True,\n",
        "              'skip_frames_env_n': 4,\n",
        "              'wrap_env': True,\n",
        "              'clip_rewards': True,\n",
        "              'episodic_life': True,\n",
        "              'scale_frame': False,\n",
        "              'stack_frames': True,\n",
        "              'warp_frames': True,\n",
        "              'warp_frames_greyscale': True,\n",
        "              }\n",
        "\n",
        "config_optim = {'optimizer': 'RMSprop',\n",
        "                'loss_criterion': 'smooth_l1_loss',  # 'huber_loss'\n",
        "                'gamma_disc': 0.99,\n",
        "                'learn_rate': 0.00025,\n",
        "                'grad_momentum': 0.95,\n",
        "                'grad_momentum_square': 0.95,\n",
        "                'min_sqr_grad': 0.01,\n",
        "                'epsilon_max': 1.00,\n",
        "                'epsilon_min': 0.10,\n",
        "                'eps_decay_interval': 1000000,\n",
        "                'exponential_decay': False,\n",
        "                'target_network_update': 10000,\n",
        "                }\n",
        "\n",
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 32,\n",
        "                 'initial_memory': 50000,\n",
        "                 }\n",
        "\n",
        "config_training = {'seed': 895359,\n",
        "                   'use_wandb_logging': True,\n",
        "                   'experiment_project_name': 'RL_ProjectTest_-_DQN_Atari_Pong-v4',\n",
        "                   'train_in_episodes': False,\n",
        "                   'max_number_training_frames': 5000000,\n",
        "                   }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsK5MUlFtewv"
      },
      "source": [
        "#### 3.1 Using network architecture from Mnih et al. (2015) with only 1 convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix-zq7cbaliY"
      },
      "outputs": [],
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 3,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_3_minimal',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 3.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a network architecture with a single conv. layer.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y25KGl1caliZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40ca579-7738-42d7-99bf-ac36a15f24e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [8:37:57<00:00, 160.89 frames/s, AvgRewardEps=-13, AvgSteps=1.43e+3, RewardMax=-2]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6WRuAvklUSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36ff4fa-c7dd-4bc0-c57b-6c4d7710498d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : -3.86 +/- 3.633\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPww-0zIYszH"
      },
      "source": [
        "#### 3.2 Using network architecture from Mnih et al. (2013) with parameters from  Mnih et al. (2015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6eqjlzFZmva"
      },
      "outputs": [],
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 1,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_1_2013',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 1.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity' +\n",
        "                                        ' and a network architecture with two conv. layers (Mnih et al., 2013).')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihpfplyNZmvi",
        "outputId": "77b37189-3ed7-4534-9fe1-87babf9dab43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [7:16:08<00:00, 191.07 frames/s, AvgRewardEps=16.8, AvgSteps=2.08e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNYLVkVllQBy",
        "outputId": "2c765935-8e8b-4cad-f6bf-24617ec79f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.44 +/- 0.92\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rwZ31Uqa2hJ"
      },
      "source": [
        "#### 3.3 Using network architecture from Mnih et al. (2015) with an additional convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 4,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_4_larger',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 4.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity' +\n",
        "                                        ' and a network architecture with four conv. layers.')\n",
        "                  )\n"
      ],
      "metadata": {
        "id": "2ouB3Jduk_L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350efd8d-78e8-443f-8d2c-db61972dea5c",
        "id": "wKuy0wUuk_MA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [8:14:02<00:00, 168.68 frames/s, AvgRewardEps=15.3, AvgSteps=1.95e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.evaluate_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6952eb-ed69-4909-cc25-64cdf5761d88",
        "id": "EiiFbsNVk_MA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.3 +/- 2.934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rotMVZexPYIV"
      },
      "source": [
        "#### 3.4 Using network architecture from Mnih et al. (2013) with parameters from  Mnih et al. (2015) and applying batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YImBisBPYIc"
      },
      "outputs": [],
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 1,\n",
        "                'use_batch_norm': True,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_1_2013_batch_norm',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 1.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity' +\n",
        "                                        ' and a network architecture with two conv. layers (Mnih et al., 2013) and batch norm.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZT3y2h8PYIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e948999f-8878-4891-b020-1f225a8b2450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [7:50:12<00:00, 177.23 frames/s, AvgRewardEps=17.7, AvgSteps=1.96e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRlC_FNEPYIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b033aaf-5119-4558-abcf-24d42e2e2ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 18.6 +/- 3.194\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Bhu6yBPrYZ"
      },
      "source": [
        "#### 3.5 Using network architecture from Mnih et al. (2015) and applying batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 2,\n",
        "                'use_batch_norm': True,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_2_2015_batch_norm_2',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity' +\n",
        "                                        ' and a network architecture with three conv. layers (Mnih et al., 2015) and batch norm.')\n",
        "                  )\n"
      ],
      "metadata": {
        "id": "q3ENmzB8PrYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a04da63-5297-4d86-f572-9c59cef18419",
        "id": "KyI9STrtf2Jk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [10:54:36<00:00, 127.30 frames/s, AvgRewardEps=17.1, AvgSteps=1.89e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.evaluate_agent()  # Evaluation time is approximately 5.7 min\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBm4UzPi5X4F",
        "outputId": "05f54fd5-2911-45ea-c276-cc5338961f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.8 +/- 1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmkotA3PYIc"
      },
      "source": [
        "#### 3.6 Using network architecture from Mnih et al. (2015) with an additional convolutional layer and applying batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 4,\n",
        "                'use_batch_norm': True,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_4_larger_batch_norm',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 4.' +\n",
        "                                        ' Using model update frequency of every 4 frames and 500k memory capacity' +\n",
        "                                        ' and a network architecture with four conv. layers and batch norm.')\n",
        "                  )\n"
      ],
      "metadata": {
        "id": "jh8BdSu3PYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwAhkXBZGOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68da10d-f2a8-4241-ccb3-6271b61ad90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] :  82%|████████▏ | 4078304/5000000 [10:56:29<3:23:00, 75.67 frames/s, AvgRewardEps=16.6, AvgSteps=1.93e+3, RewardMax=21] "
          ]
        }
      ],
      "source": [
        " scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.evaluate_agent()\n"
      ],
      "metadata": {
        "id": "ugRM6FgnPYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCyikneznJAr"
      },
      "source": [
        "#### 3.7 Using network architecture from Mnih et al. (2015) with only 1 convolutional layer and batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iorKz9VMnJAs"
      },
      "outputs": [],
      "source": [
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 3,\n",
        "                'use_batch_norm': True,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_Architecture_3_minimal_batch_norm',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 3.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a network architecture with a single conv. layer and batch norm.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DazC2uPMnJAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b83650-31f3-4502-9b76-a3d6f084b3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [7:30:23<00:00, 185.02 frames/s, AvgRewardEps=17.3, AvgSteps=1.93e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuCKdIJznJAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653ba205-2c91-470a-af9c-49303731b9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.9 +/- 0.7\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()  # 4 min to evaluate agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmCumY3MkjN"
      },
      "source": [
        "### 4 - Testing different mini-batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQrv8eGXMkjT"
      },
      "outputs": [],
      "source": [
        "config_env = {'game_id': 'Pong-v4',\n",
        "              'is_minatar': False,\n",
        "              'render_mode': 'rgb_array',\n",
        "              'max_episode_steps': None,\n",
        "              'no_op_reset_env': True,\n",
        "              'no_op_max_env': 30,\n",
        "              'skip_frames_env': True,\n",
        "              'skip_frames_env_n': 4,\n",
        "              'wrap_env': True,\n",
        "              'clip_rewards': True,\n",
        "              'episodic_life': True,\n",
        "              'scale_frame': False,\n",
        "              'stack_frames': True,\n",
        "              'warp_frames': True,\n",
        "              'warp_frames_greyscale': True,\n",
        "              }\n",
        "\n",
        "config_optim = {'optimizer': 'RMSprop',\n",
        "                'loss_criterion': 'smooth_l1_loss',  # 'huber_loss'\n",
        "                'gamma_disc': 0.99,\n",
        "                'learn_rate': 0.00025,\n",
        "                'grad_momentum': 0.95,\n",
        "                'grad_momentum_square': 0.95,\n",
        "                'min_sqr_grad': 0.01,\n",
        "                'epsilon_max': 1.00,\n",
        "                'epsilon_min': 0.10,\n",
        "                'eps_decay_interval': 1000000,\n",
        "                'exponential_decay': False,\n",
        "                'target_network_update': 10000,\n",
        "                }\n",
        "\n",
        "config_model = {'in_channels': 4,\n",
        "                'out_channel': 32,\n",
        "                'shape_input': (84, 84),\n",
        "                'kernel': (8, 8),\n",
        "                'stride': (4, 4),\n",
        "                'padding': (0, 0),\n",
        "                'out_features_linear': 512,\n",
        "                'agent_architecture': 2,\n",
        "                'use_batch_norm': False,\n",
        "                'scale_batch_input': 255.0,\n",
        "                'device': 'gpu',\n",
        "                }\n",
        "\n",
        "config_training = {'seed': 895359,\n",
        "                   'use_wandb_logging': True,\n",
        "                   'experiment_project_name': 'RL_ProjectTest_-_DQN_Atari_Pong-v4',\n",
        "                   'train_in_episodes': False,\n",
        "                   'max_number_training_frames': 5000000,\n",
        "                   }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaP0np7pMkjT"
      },
      "source": [
        "#### 4.1 Using mini-batches of 16 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPfIJDqjMkjT"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 16,\n",
        "                 'initial_memory': 50000,\n",
        "                 }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_MiniBatchSize_b=16',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a mini-batch of 16 samples.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1752ea-4f9d-423a-b319-200bcdfe363d",
        "id": "v-00vuueMkjT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [6:36:45<00:00, 210.04 frames/s, AvgRewardEps=10.8, AvgSteps=2.11e+3, RewardMax=20]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zcAnJnMkjU",
        "outputId": "8b3c7c91-c50e-4484-b2ea-91a27eebee80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 17.64 +/- 3.497\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsV7KH3LNWyG"
      },
      "source": [
        "#### 4.2 Using mini-batches of 64 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct26QG9zNWyL"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 64,\n",
        "                 'initial_memory': 50000,\n",
        "                 }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_MiniBatchSize_b=64',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a mini-batch of 64 samples.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e13425e-f4be-4819-88df-654668f592e2",
        "id": "8NugQrnZdfmc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [9:19:07<00:00, 149.04 frames/s, AvgRewardEps=17.6, AvgSteps=1.98e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2828076-31f1-46fc-9f8b-7f99a2d0701a",
        "id": "SHsIZsJ5gMd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.28 +/- 0.801\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEaozhganDJ"
      },
      "source": [
        "#### 4.3 Using mini-batches of 128 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opDEWZQeanDJ"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': 128,\n",
        "                 'initial_memory': 50000,\n",
        "                 }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_MiniBatchSize_b=128',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a mini-batch of 128 samples.')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zvspujranDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043a461c-6ab3-4735-8a76-cd84b13a499a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [23:42:32<00:00, 58.58 frames/s, AvgRewardEps=16.8, AvgSteps=1.99e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52PcvEWKanDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f4b940-e37d-4695-f39a-7b744ac6840d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.68 +/- 0.786\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTDmZnZvGkRg"
      },
      "source": [
        "#### 4.4 Using resizeable mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNFf6xQyGkRg"
      },
      "outputs": [],
      "source": [
        "config_memory = {'memory_capacity': 500000,\n",
        "                 'sample_batch_size': [16, 32, 64, 128],\n",
        "                 'initial_memory': 50000,\n",
        "                 }\n",
        "\n",
        "agent = AgentDQN(configuration_environment=config_env,\n",
        "                  configuration_dqn_models=config_model,\n",
        "                  configuration_optimization=config_optim,\n",
        "                  configuration_memory_replay=config_memory,\n",
        "                  seed=config_training['seed'],\n",
        "                  use_wandb_logging=config_training['use_wandb_logging'],\n",
        "                  experiment_project_name=config_training['experiment_project_name'],\n",
        "                  experiment_run_name='Test_MiniBatchSize_b=[16, 32, 64, 128]',\n",
        "                  experiment_run_notes=('Training run with a maximum of 5M frames and architecture 2.' +\n",
        "                                        ' Using model update frequency of every 4 frames, 500k memory capacity' +\n",
        "                                        ' and a resizable mini-batch in [16, 32, 64, 128].')\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5jaCb5BGkRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dd7670-4576-4cd3-bcea-b282237c5176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Training AgentDQN] : 100%|██████████| 5000000/5000000 [12:35:03<00:00, 110.37 frames/s, AvgRewardEps=17.2, AvgSteps=2.03e+3, RewardMax=21]\n"
          ]
        }
      ],
      "source": [
        "scores = agent.train_agent(train_in_episodes=config_training['train_in_episodes'],\n",
        "                           max_number_training_frames=config_training['max_number_training_frames'],\n",
        "                           update_frequency_model=4,\n",
        "                           render=False,\n",
        "                           render_mode='rgb_array',\n",
        "                           save_tensors_in_memory_buffer=True,\n",
        "                           load_agent_state=False,\n",
        "                           load_agent_info=None,\n",
        "                           save_iterruption=True\n",
        "                           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWO79EipGkRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32a8c2a-0cba-44a2-9dec-c3bb773adc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final average score] : 20.64 +/- 2.52\n"
          ]
        }
      ],
      "source": [
        "agent.evaluate_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.save_agent_state(base_file_name='', postfix='batch_size_resizeable')\n"
      ],
      "metadata": {
        "id": "DpT-6TEZGkRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H7fQzHLIs9gW",
        "L-dryvkoNa30",
        "jx5DgyW-NlZH",
        "uz5pXpPXORIC",
        "_Fe2XHN4QTDh",
        "focUg0hdcC13",
        "6KXnU1MMcdVN",
        "F8Z6jSQWdNiF",
        "Q_tNCS7uhBfx",
        "7qXlgGlchOp1",
        "PTKueUg8lWOI",
        "VCbZkygA5pFN",
        "Txqfo-dflIzi",
        "zHTtVsAUHUgj",
        "yd0MUPhSl3cQ",
        "c3Nyj06hqhsE",
        "SsK5MUlFtewv",
        "EPww-0zIYszH",
        "0rwZ31Uqa2hJ",
        "rotMVZexPYIV",
        "94Bhu6yBPrYZ",
        "XwmkotA3PYIc",
        "XCyikneznJAr",
        "NpmCumY3MkjN",
        "DaP0np7pMkjT",
        "QsV7KH3LNWyG",
        "pTEaozhganDJ",
        "OTDmZnZvGkRg"
      ],
      "machine_shape": "hm",
      "name": "INF8953DE_-_Atari_DQN_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}