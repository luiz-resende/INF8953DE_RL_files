# -*- coding: utf-8 -*-
"""INF8953DE_A01_-_Bandits_Dynamic-Programming.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YjKTt6cmCa1JZdoBbU5SdSLSXx1fznxf

# **INF8953DE Reinforcement Learning (Fall 2021)**

## **Assignment 01 - Bandits & Dynamic Programming**

### **Author:** Resende Silva, Luiz
### **Student ID:** 1985984

## _Python modules and general functions_
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import time
import tqdm


def plotter_func(xy_data, x_label="X Axis", y_label="Y Axis", graph_title="My Plot", use_x_limits=False, x_limits=(0, 100),
                 use_y_limits=False, y_limits=(0, 100), use_x_ticks=False, x_ticks=1, use_y_ticks=False, y_ticks=1,
                 use_log_scale=False, figure_size=[15, 10], title_font_size=26, save_plot=False, save_directory="same"):
    """
    Function created to graphically represent the results of f(x) (up to 28 different functions) for a set of values x=(1,...,n).

    Parameters
    ----------
    xy_data : pandas.DataFrame
        Structure with labelled xy data points, where first column contains the x points and the subsequent columns the different f(x) values.
    x_label : str, optional
        Label for the x axis. The default is "X Axis".
    y_label : str, optional
        Label for the y axis. The default is "Y Axis".
    graph_title : str, optional
        Title for the plot. The default is "My Plot".
    use_x_limits : bool, optional
        Boolean flag to whether or not use preset minimum and maximum values for x axis. The default is False.
    x_limits : tuple, optional
        Tuple with the minimum and maximum limit values for the x axis. The default is (0, 100).
    use_y_limits : bool, optional
        Boolean flag to whether or not use preset minimum and maximum values for y axis. The default is False.
    y_limits : tuple, optional
        Tuple with the minimum and maximum limit values for the y axis. The default is (0, 100).
    use_x_ticks : bool, optional
        Boolean flag to whether or not use predefined minimum x axis increment unit. The default is False.
    x_ticks : int, optional
        X axis increment unit. The default is 1.
    use_y_ticks : bool, optional
        Boolean flag to whether or not use predefined minimum y axis increment unit. The default is False.
    y_ticks : int, optional
        Y axis increment unit. The default is 1.
    use_log_scale : bool, optional
        Boolean flag to whether or not use logarithm scale for the x axis. The default is False.
    figure_size : list, optional
        List with two int values representing figure size [width, hight]. The default is [15, 10].
    title_font_size : int, optional
        Title font size, which is also used for the axes title sizes (decreased by 4 units). The default is 26.
    save_plot : bool, optional
        Boolean flag to whether or not save the plot as a PNG file. The default is False.
    save_directory : str, optional
        Either "same" for saving in the same folder as the script or "C:\\...\\...\\..." directory where to save figures. The default is "".

    Raises
    ------
    TypeError
        Argument 'xy_data' passed is not a pandas.DataFrame!
    ValueError
        Argument 'xy_data' passed has more then 28 f(x) columns...
    """
    try:
        if (not isinstance(xy_data, pd.DataFrame)):
            raise TypeError("TypeError: Argument 'xy_data' passed is not a pandas.DataFrame!")
        columns = list(xy_data.columns.values)
        color_list = ['red', 'green', 'blue', 'yellow', 'magenta', 'black', 'cyan']
        line_types = ['-', '-.', ':', '--']
        marker_types = ['.', 'o', 'v', 'p', 'D', 's', '+']
        if ((len(columns) - 1) > 28):
            raise ValueError("ValueError: Argument 'xy_data' passed has more then 28 f(x) columns...")
        fig, ax = plt.subplots(figsize=figure_size)
        ax.grid(True)
        if (use_log_scale is True):
            ax.set_xscale('log')
        if (len(columns) < 9):
            for i in range(len(columns) - 1):
                plt.plot(columns[0], columns[i + 1], data=xy_data, marker='', linestyle=line_types[0], markersize=None,
                         color=color_list[i], linewidth=2)
        else:
            for i in range(len(columns) - 1):
                if (i < 7):
                    plt.plot(columns[0], columns[i + 1], data=xy_data, marker='', linestyle=line_types[0],
                             markerfacecolor=None, markersize=None, color=color_list[i], linewidth=2)
                elif ((i >= 7) and (i < 14)):
                    plt.plot(columns[0], columns[i + 1], data=xy_data, marker='', linestyle=line_types[1],
                             markerfacecolor=None, markersize=None, color=color_list[i - 7], linewidth=2)
                    if((i + 1) == (len(columns) - 1)):
                        break
                elif ((i >= 14) and (i < 21)):
                    plt.plot(columns[0], columns[i + 1], data=xy_data, marker='', linestyle=line_types[2],
                             markerfacecolor=None, markersize=None, color=color_list[i - 14], linewidth=2)
                    if((i + 1) == (len(columns) - 1)):
                        break
                elif ((i >= 21) and (i < 28)):
                    plt.plot(columns[0], columns[i + 1], data=xy_data, marker='', linestyle=line_types[3],
                             markerfacecolor=None, markersize=None, color=color_list[i - 21], linewidth=2)
                    if((i + 1) == (len(columns) - 1)):
                        break
        plt.suptitle(graph_title, fontsize=title_font_size)
        if(x_label == "X Axis"):
            x_label = columns[0]
        plt.xlabel(x_label, fontsize=(title_font_size - 4))
        plt.ylabel(y_label, fontsize=(title_font_size - 4))
        if (use_x_limits):
            plt.xlim(x_limits[0], x_limits[1])
            if(use_x_ticks):
                ax.xaxis.set_ticks(np.arange(x_limits[0], x_limits[1], x_ticks))
        elif (use_x_ticks):
            ax.xaxis.set_ticks(np.arange(xy_data[columns[0]].min(), xy_data[columns[0]].max(), x_ticks))
        if (use_y_limits):
            plt.ylim(y_limits[0], y_limits[1])
            if (use_y_ticks):
                ax.yaxis.set_ticks(np.arange(y_limits[0], y_limits[1], y_ticks))
        elif (use_y_ticks):
            ax.yaxis.set_ticks(np.arange(xy_data.iloc[:, 1:].min().min(), xy_data.iloc[:, 1:].max().max(), y_ticks))
        plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, fontsize='xx-large')
        plt.show()
        if (save_plot):
            timestr = time.strftime("%y-%m-%d_%Hh%Mm%Ss_")
            file_name = graph_title.replace('\\', "").replace('$', "").replace(' ', "").replace('vs.', "").replace(':', "_")
            if (save_directory == "same"):
                plt.savefig(timestr + file_name + ".png")
            else:
                plt.savefig(save_directory + "/" + timestr + file_name + ".png")
    except Exception as e:
        if (type(e) == TypeError or type(e) == ValueError):
            print(e)
        else:
            print("UnknownError: Problem while running 'plotter_func()'. Please, review arguments passed...")
            print("ERROR MESSAGE: %s" % (e))


def find_uniques_idx(arr, sort_type="mergesort"):
    """
    Method to find duplicate elements in an array and return a list of tuples (x, y), where x is the duplicated value and
    y is a list of indices where they accurr in the array.

    Parameters
    ----------
    arr : list or numpy.ndarray
        Array of elements to be tested.
    sort_type : str, optional
        The sorting method to be used in numpy.argsort(). The default is "mergesort".

    Returns
    -------
    repeats : dict
        Dictionary containing duplicate value as key and a list with their respective indices of appearence as key's value.

    Raises
    ------
    TypeError
        The array passed is of a type not supported. Expected type list or type numpy.ndarray and got: type(arr)
        Expected an array of dimension 1, got array of dimension arr.ndim
    """
    try:
        if (type(arr) == list):
            arr = np.array(arr)
        elif (type(arr) != np.ndarray):
            raise TypeError("TypeError: The array passed is of a type not supported. Expected type list or type numpy.ndarray and got: ", type(arr))
        if (arr.ndim > 1):
            raise TypeError("TypeError: Expected an array of dimension 1, got array of dimension ", arr.ndim)
        lst = np.array(range(len(arr)))
        arr = np.array([arr, lst]).T  # Adding column with initial indices
        idx_argsort = np.argsort(arr[:, 0], kind=sort_type)  # Sorting values to have repeats grouped together
        sorted_arr = arr[idx_argsort, :]
        # Returning the unique values, the index of the first occurrence of a value, and the count for each element
        vals, idx_start, count = np.unique(sorted_arr[:, 0], return_counts=True, return_index=True)
        # Remove unique values
        vals = vals[count > 1]
        idx_start = idx_start[count > 1]
        count = count[count > 1]
        repeats = dict()
        # Looking through repeats and retrieving indices
        for v, i, c in zip(vals, idx_start, count):
            idxs = [sorted_arr[i, 1], sorted_arr[i + 1, 1]]
            if c > 2:
                for k in range(2, c):
                    idxs.append(sorted_arr[i + k, 1])
            repeats[v] = idxs
        return repeats
    except Exception as e:
        print(e)


# %%


"""## QUESTION 1 - Bandit Problem

### 1.1 - ```class ArmedBandit()```

The below Python 3.7 class ```ArmedBandit()``` implements the 10-armed testbed as indicated in the [Assignment 01 instructions](https://drive.google.com/file/d/1eE9EJcyf1K9niiosnGO7vvjgQtkLJWz2/view). Its constructor initializes the necessary variables and accepts modifications in the number of arms $k$, number of pulls $T$ and exploration probability $\epsilon$. Methods ```ArmedBandit.choose()``` chooses an action $A_{t}$ at time $t$ and ```ArmedBandit.pull()``` takes chosen action $A_{t}$ and returns the respective reward $R_{t}(A_{t})=N(q_{*}(A_{t}), 1)$. The method ```ArmedBandit.play()``` starts the learning iterations from $1$ to $T$ and updates the estimates $Q_{t}(a)$ and cumulative reward $\sum_{i=1}^{t}{R}_{i}$ at step $t$.
"""


class ArmedBandit():
    """
    Class implementing the multi-armed bandit problem with the e-Greedy, Optimistic Initial Values, Upper-Confidence-Bound Action Selection
    and Gradient Bandit methods, which are controlled by the arguments ```epsilon```, ```optimistic_init_value```, ```ucb_c``` and
    ```gradient_alpha```, respectively. These methods can be used individually or combined. To use a combination of method, the previous shown arguments
    must be simply set to values different than zero. If all method-related parameters are kept to zero, the class ```ArmedBandit()``` will
    run a purelly greedy action selection algorithm.

    Attributes
    ----------
    number_of_arms : int
        Integer for the number of arms/actions.
    number_of_pulls : int
        Integer value for the number of pulls/steps.
    number_of_runs : int
        Integer value for the number of independent runs.
    q_star_mean : float
        Mean to sample normal distribution for $q_{*}(a)$ true values.
    q_star_std_dev : float
        Standart deviation to sample normal distribution for $q_{*}(a)$ true values.
    seeds_list : list or numpy.ndarray or None
        List of seeds for the defaults random number generators used in the class
    pull_all_arms_once : bool
        Flag to whether or not initialize the algorithm selecting all actions once

    epsilon : float
        Exploration probability $\\epsilon$ for e-greedy method.
    optimistic_init_value : float
        Positive value for the Positive Initial Optimistic Value method.
    ucb_c : float
        Upper-Confidence-Bound (UCB) degree of exploration parameter.
    gradient_alpha : float
        Gradient bandit step-size parameter.
    use_reward_baseline : bool
        Whether or not to use the average of all rewards as reward baseline in gradient bandit.
    theta : float
        A very small number used for numerical stabilization of UCB algorithms.

    rand_gen : list
        List of ```numpy.random.default_rng()``` for reproducibility.
    actions : numpy.ndarray
        List with indices for the $k$ arms/actions.
    q_a_star : numpy.ndarray
        2D array for the $k$ true action values over the $n$ runs.
    q_a_estimate : numpy.ndarray
        2D array for the $k$ estimated action values over the $n$ runs.
    times_action : numpy.ndarray
        2D array for number of times each of the $k$ actions were chosen over the $n$ runs.
    reward_action : numpy.ndarray
        2D array for the cumulative reward for each of the $k$ actions over the $n$ runs.
    reward_steps : numpy.ndarray
        2D array with the reward achieved at each step $t$ and run $n$.
    reward_cumulative : numpy.ndarray
        Array of size $n$ for the total cumulative reward over each run.
    reward_average : numpy.ndarray
        2D array with the average reward at each step $t$ for the $n$ runs.
    action_preferences : numpy.ndarray
        2D array with the action preferences at each run $n$ for the $k$ arms.
    pi_action_prob : numpy.ndarray
        2D array with the action taking probabilities at each run $n$ for the $k$ arms.
    total_avg_reward : numpy.ndarray
        Array of size $T$ with the average reward at each step $t$ over the $N$ runs.
    total_avg_regret : numpy.ndarray
        Array of size $T$ with the total average regret at each step $t$ over the $N$ runs.
    total_opt_percent : numpy.ndarray
        Array of size $T$ with the percentage of time the optimal action is taken at each step $t$ over the $N$ runs.

    Methods
    -------
    __update_attributes(run, step, act_t, rew_t, incremental=False):
        Updates to update the class attributes related to the reward $R_{t}$ at step $t$, cumulative action reward,
        cumulative total reward, total reward average and new estimates $Q_{t}(a)$. Also calculates and updates
        action preferences $H_{t+1}(A_{t})$ at step $t$ for step $t+1$ if argument ```gradient_alpha>0```.
    __update_gradient_action_probability(self, run):
        Calculates the probability $Prob{A_{t}=a}$ of taking action $a$ at step $t$.
    action_selection(run, step):
        Chooses the action $A_{t}$ to be taken at time $t$ on run $n$.
    pull(run, action):
        Calculates the reward received from taking action $A_{t}$ at time $t$ on run $n$.
    play():
        Plays armed bandit through defined number of pull times and runs.
    get_metrics():
        Calculates the average regret at each pull/step $t$ over the $N$ runs.
    """

    def __init__(self, number_of_arms=10, number_of_pulls=1000, number_of_runs=2000, q_star_mean=0.0, q_star_variance=1.0,
                 seeds_list=None, pull_all_arms_once=False, epsilon=0.0, optimistic_init_value=0.0, ucb_c=0.0,
                 gradient_alpha=0.0, use_reward_baseline=False, theta=0.000001):
        """
        Constructs all the necessary attributes for the ArmedBandit object.

        Parameters
        ----------
        number_of_arms : int, optional
            Number $k$ of arms/actions. The default is 10.
        number_of_pulls : int, optional
            Number of pulls/steps in each run. The default is 1000.
        number_of_runs : int, optional
            Number of independent runs. The default is 20.
        q_star_mean : float
            Mean to sample normal distribution for $q_{*}(a)$ true values.
        q_star_variance : float
            Variance to sample normal distribution for $q_{*}(a)$ true values.
        seeds_list : list or numpy.ndarray or None, optional
            List of seeds for the defaults random number generators used in the class. The default is None.
        pull_all_arms_once : bool
            Flag to whether or not initialize the algorithm selecting all actions once. The default is False.
        epsilon : float, optional
            Exploration probability $\\epsilon$ for e-greedy method. The default is 0.0.
        optimistic_init_value : float, optional
            Optimistic initial value to be applied in the initial estimates. The default is 0.0.
        ucb_c : float
            Upper-Confidence-Bound (UCB) degree of exploration parameter. The default is 0.0.
        gradient_alpha : float
            Gradient bandit step-size parameter. The default is 0.0.
        use_reward_baseline : bool
            Flag to whether or not use the average of all rewards as reward baseline in gradient bandit (```gradient_alpha>0```). The default is False.
        theta : float
            A very small number used for numerical stabilization of UCB algorithm. The default is 1e-6.
        """
        self.number_of_arms = number_of_arms
        self.number_of_pulls = number_of_pulls
        self.number_of_runs = number_of_runs
        self.q_star_mean = q_star_mean
        self.q_star_std_dev = np.sqrt(q_star_variance)
        self.seeds_list = seeds_list
        self.pull_all_arms_once = pull_all_arms_once
        # Action selection method attributes
        self.epsilon = epsilon
        self.optimistic_init_value = optimistic_init_value
        self.ucb_c = ucb_c
        self.gradient_alpha = gradient_alpha
        self.use_reward_baseline = use_reward_baseline
        self.theta = theta
        # Variable attributes
        self.seeds_list_2 = list()
        self.rand_gen = list()
        self.actions = list()
        self.q_a_star = list()
        self.q_a_estimate = list()
        self.times_action = list()
        self.reward_action = list()
        self.reward_steps = list()
        self.reward_cumulative = list()
        self.reward_average = list()
        self.action_preferences = list()
        self.pi_action_prob = list()
        self.total_avg_reward = list()
        self.total_avg_regret = list()
        self.total_opt_percent = list()
        self.chosen_actions = list()

    def __initialize_variables(self):
        """
        Private method to initialize/reset the class variables
        """
        self.seeds_list_2 = list(np.zeros(self.number_of_runs, dtype=int))
        self.rand_gen = list(np.zeros(self.number_of_runs, dtype=int))
        self.actions = np.arange(self.number_of_arms)
        self.q_a_star = np.zeros((self.number_of_runs, self.number_of_arms), dtype=float)
        self.q_a_estimate = np.ones((self.number_of_runs, self.number_of_arms), dtype=float) * self.optimistic_init_value
        self.times_action = np.zeros((self.number_of_runs, self.number_of_arms), dtype=int)
        self.reward_action = np.zeros((self.number_of_runs, self.number_of_arms), dtype=float)
        self.reward_steps = np.zeros((self.number_of_runs, self.number_of_pulls), dtype=float)
        self.reward_cumulative = np.zeros(self.number_of_runs, dtype=float)
        self.reward_average = np.zeros((self.number_of_runs, self.number_of_pulls), dtype=float)
        self.chosen_actions = np.zeros((self.number_of_runs, self.number_of_pulls), dtype=int)
        self.action_preferences = np.zeros((self.number_of_runs, self.number_of_arms), dtype=float)
        self.pi_action_prob = (np.ones((self.number_of_runs, self.number_of_arms), dtype=float) / self.number_of_arms)

    def __update_attributes(self, run, step, act_t, rew_t, incremental=False):
        """
        Private method to update the class attributes related to the reward $R_{t}$ at step $t$, cumulative action reward,
        cumulative total reward, total reward average and new estimates $Q_{t}(a)$. Method also calculates and updates
        action preferences $H_{t+1}(A_{t})$ at step $t$ for step $t+1$ if argument ```gradient_alpha>0```.

        Parameters
        ----------
        run : int
            Integer representing the current $n^{th}$ run.
        step : int
            Integer representing the current $t^{th}$ step.
        act_t : int
            Integer representing the action $A_{t}$ taken at step $t$.
        rew_t : float
            The reward $R_{t}$ generated from choosing action $A_{t}$ at step $t$.
        incremental : bool
            Whether of not to use incremental method to calculate new estimates. The default is False.

        Return
        ------
        None
        """
        self.reward_steps[run, step] += rew_t
        self.reward_action[run, act_t] += rew_t
        self.reward_cumulative[run] += rew_t
        self.reward_average[run, step] = (self.reward_cumulative[run] / (step + 1))

        if (incremental):
            self.q_a_estimate[run, act_t] = self.q_a_estimate[run, act_t] + ((rew_t - self.q_a_estimate[run, act_t]) / (step + 1))
        else:
            self.q_a_estimate[run, act_t] = (self.reward_action[run, act_t] / self.times_action[run, act_t])

        if (self.gradient_alpha > 0.0):
            not_act_t = self.actions[self.actions != act_t]
            if (self.use_reward_baseline):
                self.action_preferences[run, act_t] += (self.gradient_alpha * (rew_t - self.reward_average[run, step]) * (1 - self.pi_action_prob[run, act_t]))
                self.action_preferences[run, not_act_t] -= (self.gradient_alpha * (rew_t - self.reward_average[run, step]) * self.pi_action_prob[run, not_act_t])
            else:
                self.action_preferences[run, act_t] += (self.gradient_alpha * rew_t * (1 - self.pi_action_prob[run, act_t]))
                self.action_preferences[run, not_act_t] -= (self.gradient_alpha * rew_t * self.pi_action_prob[run, not_act_t])
        return None

    def __update_gradient_action_probability(self, a_preferences):
        """
        Private method to calculate the probability $Prob{A_{t}=a}$ of taking action $a$ at step $t$ based on current action
        preferences $H_{t}(a)$. For numerical stability, $\\max{H_{t}(a)}$ is subtracted from both the numerator and
        denominator.

        Parameters
        ----------
        a_preferences : numpy.ndarray
            Vector with the action preferences $H_{t}(a)$ for the different actions $a$.

        Return
        ------
        a_probabilities : numpy.ndarray
            Vector with the action probabilities $\\pi_{t}(a)$ for the different actions $a$.
        """
        a_probabilities = np.exp(a_preferences - np.max(a_preferences)) / np.sum(np.exp(a_preferences - np.max(a_preferences)), axis=0)
        return a_probabilities

    def pull(self, run, action):
        """
        Method to calculate the reward received from taking action $A_{t}$ at time $t$ on run $n$.

        Parameters
        ----------
        action : int
            Integer in the range (0, self.number_of_arms] representing the action $A_{t}$ taken.
        run : int
            $n^{th}$ problem index.

        Raises
        ------
        ValueError
            The input action is not recognized! Please, chose a value in the interval (0, %d]

        Return
        ------
        reward : float
            Reward $R_{t}$ for action $A_{t}$ taken.
        """
        if (action >= self.number_of_arms or action < 0):
            raise ValueError("ValueError: The input action is not recognized! Please, chose a value in the interval (0, %d]" % (self.number_of_arms))
        self.times_action[run, action] += 1
        reward = self.rand_gen[run].normal(self.q_a_star[run, action], 1)
        return reward

    def action_selection(self, run, step):
        """
        Method to choose the action $A_{t}$ to be taken at time $t$ on run $n$. Chooses an action using either greedy, e-Greedy,
        Optimistic Initial Values, Upper-Confidence-Bound or Gradient Bandit.

        Parameters
        ----------
        run : int
            $n^{th}$ problem index.
        step : int
            The $t^{th}$ step where the action $A_{t}$ must be chosen.

        Return:
        -------
        action : int
            Integer in the range (0, self.number_of_arms] representing the action $A_{t}$ to be taken.
        """
        prob_explr = self.rand_gen[run].random()
        if (prob_explr < self.epsilon):
            action = self.rand_gen[run].choice(self.actions)
        else:
            if (self.ucb_c > 0.0):
                if ((step - 1) > 0):
                    ucb_Q_a = self.q_a_estimate[run, :] + (self.ucb_c * np.sqrt((np.log(step) / (self.times_action[run, :] + self.theta))))
                    action = np.argmax(ucb_Q_a)
                else:
                    action = self.rand_gen[run].choice(self.actions)
            elif (self.gradient_alpha > 0.0):
                self.pi_action_prob[run] = self.__update_gradient_action_probability(self.action_preferences[run])
                action = self.rand_gen[run].choice(self.actions, p=self.pi_action_prob[run])
            else:
                action = np.argmax(self.q_a_estimate[run, :])
        self.chosen_actions[run][step - 1] = action
        return action

    def play(self, use_incremental_update=False):
        """
        Method to play armed bandit through defined number of pull times and runs.

        Return
        ------
        self.total_avg_reward : numpy.ndarray
            Array of size $T$ containing the average reward at each step $t$ over the $N$ independent runs.
        """
        self.__initialize_variables()
        # START LOOPING THROUGH THE NUMBER OF RUNS
        for n in tqdm.tqdm(range(self.number_of_runs)):
            time.sleep(0.0)
            if ((type(self.seeds_list) == list) or (type(self.seeds_list) == np.ndarray)):
                if (len(self.seeds_list) < self.number_of_runs and len(self.seeds_list) != n):
                    raise ValueError("ValueError: Length of list argument ```self.seeds_list``` different from argument ```self.number_of_runs```...")
                self.rand_gen[n] = np.random.default_rng(self.seeds_list[n])
            else:
                seed = np.random.default_rng().integers(1, ((2**63 - 1) - (2**32)), size=1)
                self.seeds_list_2[n] = seed
                self.rand_gen[n] = np.random.default_rng(seed)
            # INITIALIZE TRUE ACTION VALUES
            self.q_a_star[n] = self.rand_gen[n].normal(self.q_star_mean, self.q_star_std_dev, self.number_of_arms)
            # STARTING LEARNING ITERATIONS FOR CURRENT RUN
            init_t = 0
            # IF ALL ARMS ARE TO BE PULLED ONCE
            if (self.pull_all_arms_once is True):
                lst_actions = np.array(range(self.number_of_arms))
                self.rand_gen[n].shuffle(lst_actions)
                for temp_t, act in enumerate(lst_actions):
                    rew = self.pull(n, act)
                    self.__update_attributes(n, temp_t, act, rew, use_incremental_update)
                init_t = len(lst_actions)
            for t in range(init_t, self.number_of_pulls):
                action_t = self.action_selection(n, (t + 1))
                reward_t = self.pull(n, action_t)
                self.__update_attributes(n, t, action_t, reward_t, use_incremental_update)
        self.total_avg_reward = np.average(self.reward_average, axis=0)
        return self.total_avg_reward

    def get_metrics(self, get_optimal_percent=False):
        """
        Method to calculate the average regret at each pull/step $t$ over the $N$ runs.

        Parameters
        ----------
        get_optimal_percent : bool, optional
            Flag defining whether to return average regret or average average percentage of times the optimal arm is selected.

        Return
        ------
        metrics : numpy.ndarray
            Array of size $T$ with total average regret at each step $t$ over the $N$ independent runs.
        """
        number_steps = np.array(list(range(1, self.number_of_pulls + 1)))
        opt_idx = np.argmax(self.q_a_star, axis=1)
        optimal_values = np.array([self.q_a_star[i, opt_idx[i]] for i in range(self.q_a_star.shape[0])])
        if (get_optimal_percent):
            percent_opt = []
            for n in range(self.number_of_runs):
                temp_percent = []
                for t in range(self.number_of_pulls):
                    temp_percent.append(1.0 if (self.chosen_actions[n][t] == opt_idx[n]) else 0.0)
                percent_opt.append(temp_percent)
            self.total_opt_percent = np.average(np.array(percent_opt, dtype=float), axis=0) * 100.0
            metrics = self.total_opt_percent
        else:
            regrets = []
            for n in range(self.number_of_runs):
                temp_reg = []
                for t in range(self.number_of_pulls):
                    temp_reg.append(((optimal_values[n] * number_steps[t]) - self.reward_steps[n, 0:(t + 1)].sum()))
                regrets.append(temp_reg)
            self.total_avg_regret = np.average(np.array(regrets), axis=0)
            metrics = self.total_avg_regret
        return metrics


"""### _Seeds_"""

seeds = pd.read_csv(r"https://raw.githubusercontent.com/luiz-resende/INF8953DE_RL_files/main/A01_Bandits_DynamicProgramming/seeds2000.csv", sep=',', header=None).to_numpy().tolist()

step_list = range(1, 1001)

number_runs = 500

# %%


"""### 1.2 - Running $\epsilon$-greedy Method

The class ```ArmedBandit()``` above was implemented including an exploration probability argument ```epsilon``` into its constructor. Such paramater is used to perform runs using $\epsilon$-Greedy algorithm, where actions $A_{t}$ will be selected randomly with probility $\epsilon$ and selected greedly with probability $(1-\epsilon)$. $\epsilon=\{0.00, 0.01, 0.10\}$.

#### _Code_
"""

bandit_eps = np.zeros((4, 3), dtype=int).tolist()
eps = [0.00, 0.01, 0.10, 0.90]

for i in range(len(eps)):
    bandit_eps[i][0] = ArmedBandit(number_of_runs=number_runs, seeds_list=seeds[i], pull_all_arms_once=False,
                                   epsilon=eps[i])
    bandit_eps[i][1] = bandit_eps[i][0].play(use_incremental_update=False)
    bandit_eps[i][2] = bandit_eps[i][0].get_metrics()

avg_reward_data = pd.DataFrame(np.array([step_list, bandit_eps[0][1], bandit_eps[1][1], bandit_eps[2][1]]).T,
                               columns=[r'Times $t$', (r'$\epsilon=' + str(eps[0]) + r'$'),
                                        (r'$\epsilon=' + str(eps[1]) + r'$'), (r'$\epsilon=' + str(eps[2]) + r'$')])
plotter_func(avg_reward_data, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 1: $\epsilon$-greedy Method - Average Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

avg_regret_data = pd.DataFrame(np.array([step_list, bandit_eps[0][2], bandit_eps[1][2], bandit_eps[2][2]]).T,
                               columns=[r'Times $t$', (r'$\epsilon=' + str(eps[0]) + r'$'),
                                        (r'$\epsilon=' + str(eps[1]) + r'$'), (r'$\epsilon=' + str(eps[2]) + r'$')])
plotter_func(avg_regret_data, x_label='Number pulls ($t$)', y_label=('Average Total Regret (%d runs)' % (number_runs)),
             graph_title=r'Fig. 2: $\epsilon$-greedy Method - Average Total Regret vs. Number Pulls', save_plot=False, save_directory=r"same")

# %%


# a = np.array([bandit_eps[0].seeds_list_2, bandit_eps[1].seeds_list_2, bandit_eps[2].seeds_list_2])
# df = pd.DataFrame(a)
# df.to_csv(r"D:\Users\luiza\Downloads\123.csv", index=False)

# %%


"""### 1.3 - Using $\epsilon=0.9$

Next, the algorithm was initialized using the $\epsilon$-Greedy Method with $\epsilon=0.90$, which entails in the algorithm will selecting actions $A_{t}$ randomly with $90\%$ probability and results in exploiting the actions only at most $19\%$ of times. It is than evident that such high probability of exploration will hurt the algorithms ability to exploit higher return actions and decrease the expected average rewards achieved over the total $T$ pulls. Moreover, it is expected that the total average regret for $\epsilon=0.90$ over $T$ pulls will be much higher when compared to $\epsilon=\{0.00, 0.01, 0.10\}$ given its lower rewards achieved.

As expected, the lower average rewards and higher average regrets can be easily notice in [Figure 3](#Figure03) and [Figure 4](#Figure04) below.

#### _Code_

<a id="Figure03"></a>
"""

avg_reward_data_eps90 = pd.DataFrame(np.array([step_list, bandit_eps[3][1]]).T,
                                     columns=[r'Times $t$', (r'$\epsilon=' + str(eps[3]) + r'$')])
plotter_func(avg_reward_data_eps90, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 3: $\epsilon$-greedy Method - Average Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

"""<a id="Figure04"></a>"""

avg_regret_data_eps90 = pd.DataFrame(np.array([step_list, bandit_eps[3][2]]).T,
                                     columns=[r'Times $t$', r'$\epsilon=' + str(eps[3]) + '$'])
plotter_func(avg_regret_data_eps90, x_label='Number pulls ($t$)', y_label=('Average Total Regret (%d runs)' % (number_runs)),
             graph_title=r'Fig. 4: $\epsilon$-greedy Method - Average Total Regret vs. Number Pulls', save_plot=False, save_directory=r"same")

# %%


"""### 1.4 - Optimistic Initial Value

The ```class ArmedBandit()``` is further extended to included Optimistic Initial Value method, where the initial action-value estimates are set to be greater than zero $(Q_{1}(a)>0)$, through the argument ```optimistic_init_value```. The method is implemented such that all actions $a$ have the same initial action-value estimate.

Optimistic Initial Values help the algorithm to scape the exploitation bias imposed by initial action-value estimates set to zero $(Q_{1}(a)=0)$ and encourage exploration of different actions during the initial steps. Therefore, one it is right to expect that setting $Q_{1}(a)>0$ would result in higher average rewards, given the initial positive action-value estimates induced exploration will allow testing actions with better returns for later exploitation. This higher average reward will in turn result in lower total average regrets.

As expected, [Figure 5](#Figure05) clearly shows that, even though $Q_{1}(a)=0$ appears to performe better than $Q_{1}(a)=+3$, the later preduces results where the average reward after $T$ steps is over 2 times higher than the former. This initial good performance of $Q_{1}(a)=0$ over $Q_{1}(a)=+3$ in the first 20 steps is given by greedy approach of $Q_{1}(a)=0$ from the start. However, using $Q_{1}(a)=+3$ allows to explore the actions in these first steps and later exploit the ones found to have better returns.

#### _Code_
"""

bandit_oiv = np.zeros((3, 3), dtype=int).tolist()
oiv = [0.0, 3.0, 100.0]

for i in range(len(oiv)):
    bandit_oiv[i][0] = ArmedBandit(number_of_runs=number_runs, seeds_list=seeds[i], pull_all_arms_once=False, epsilon=0.00,
                                   optimistic_init_value=oiv[i])
    bandit_oiv[i][1] = bandit_oiv[i][0].play()
    bandit_oiv[i][2] = bandit_oiv[i][0].get_metrics()

"""<a id="Figure05"></a>"""

avg_reward_data_oiv = pd.DataFrame(np.array([step_list, bandit_oiv[0][1], bandit_oiv[1][1]]).T,
                                   columns=[r'Times $t$', (r'$Q_{1}(a)=+' + str(oiv[0]) + r'$'),
                                            (r'$Q_{1}(a)=+' + str(oiv[1]) + r'$')])
plotter_func(avg_reward_data_oiv, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 5: Optimistic Initial Value - Average Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

"""<a id="Figure06"></a>"""

avg_regret_data_oiv = pd.DataFrame(np.array([step_list, bandit_oiv[0][2], bandit_oiv[1][2]]).T,
                                   columns=[r'Times $t$', (r'$Q_{1}(a)=+' + str(oiv[0]) + r'$'),
                                            (r'$Q_{1}(a)=+' + str(oiv[1]) + r'$')])
plotter_func(avg_regret_data_oiv, x_label='Number pulls ($t$)', y_label=('Average Total Regret (%d runs)' % (number_runs)),
             graph_title=r'Fig. 6: Optimistic Initial Value - Average Total Regret vs. Number Pulls', save_plot=False, save_directory=r"same")

# %%


"""### 1.5 - Optimistic Initial Value of $+100$

#### _Code_

<a id="Figure08"></a>
"""

avg_reward_data_oiv = pd.DataFrame(np.array([step_list, bandit_oiv[0][1], bandit_oiv[1][1], bandit_oiv[2][1]]).T,
                                   columns=[r'Times $t$', (r'$Q_{1}(a)=+' + str(oiv[0]) + r'$'),
                                            (r'$Q_{1}(a)=+' + str(oiv[1]) + r'$'), (r'$Q_{1}(a)=+' + str(oiv[2]) + r'$')])
plotter_func(avg_reward_data_oiv, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 7: Optimistic Initial Value: Avrg Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

"""<a id="Figure08"></a>"""

avg_regret_data_oiv = pd.DataFrame(np.array([step_list, bandit_oiv[0][2], bandit_oiv[1][2], bandit_oiv[2][2]]).T,
                                   columns=[r'Times $t$', (r'$Q_{1}(a)=+' + str(oiv[0]) + r'$'),
                                            (r'$Q_{1}(a)=+' + str(oiv[1]) + r'$'), (r'$Q_{1}(a)=+' + str(oiv[2]) + r'$')])
plotter_func(avg_regret_data_oiv, x_label='Number pulls ($t$)', y_label=('Average Total Regret (%d runs)' % (number_runs)),
             graph_title=r'Fig. 8: Optimistic Initial Value - Avrg. Total Regret vs. Number Pulls', save_plot=False, save_directory=r"same")

# %%


r"""### 1.6 - Upper-Confidence-Bound (UCB)

Next, the ```class ArmedBandit()``` is extended to included Upper-Confidence-Bound (UCB) action selection method, where the action $A_{t}$ will be selected at step $t$ following

\begin{equation}
A_{t}={\underset{a}{\operatorname{argmax}}}\left[Q_{t}(a)+c{\sqrt{\frac{ln(t)}{N_{t}(a)}}}\right]
\end{equation}

with paramter $c$ (degree of exploration) being initialized in the class object instantiation using argument ```ucb_c```.

It is expected that increasing $c$ will also increase the algorithm's performance in terms of expected average reward, once this will lead to exploration of the available actions. However, one can assume that a high $c$ might encourage too much exploration and undermine the exploitation of good actions.

From [Figure 9](#Figure09) it is possible to confirm the expectations regarding setting parameter $c$ to a high value, where one can see that $c=5.0$ underperforms the other two settings, not having stabilized even after $1000$ steps.

#### _Code_
"""

bandit_ucb = np.zeros((3, 3), dtype=int).tolist()
ucb = [0.2, 1.0, 5.0]

for i in range(len(ucb)):
    bandit_ucb[i][0] = ArmedBandit(number_of_runs=number_runs, seeds_list=seeds[i], pull_all_arms_once=False, ucb_c=ucb[i])
    bandit_ucb[i][1] = bandit_ucb[i][0].play()
    bandit_ucb[i][2] = bandit_ucb[i][0].get_metrics()

"""<a id="Figure09"></a>"""

avg_reward_data_ucb = pd.DataFrame(np.array([step_list, bandit_ucb[0][1], bandit_ucb[1][1], bandit_ucb[2][1]]).T,
                                   columns=[r'Times $t$', (r'$c=' + str(ucb[0]) + r'$'), (r'$c=' + str(ucb[1]) + r'$'),
                                            (r'$c=' + str(ucb[2]) + r'$')])
plotter_func(avg_reward_data_ucb, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 9: UCB - Average Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

"""<a id="Figure10"></a>"""

avg_regret_data_ucb = pd.DataFrame(np.array([step_list, bandit_ucb[0][2], bandit_ucb[1][2], bandit_ucb[2][2]]).T,
                                   columns=[r'Times $t$', (r'$c=' + str(ucb[0]) + r'$'), (r'$c=' + str(ucb[1]) + r'$'),
                                            (r'$c=' + str(ucb[2]) + r'$')])
plotter_func(avg_regret_data_ucb, x_label='Number pulls ($t$)', y_label=('Average Total Regret (%d runs)' % (number_runs)),
             graph_title=r'Fig. 10: UCB - Average Total Regret vs. Number Pulls', save_plot=False, save_directory=r"same")

# %%


"""### 1.7 - Gradient Bandit

Lastly, class ```ArmedBandit()``` implements the Gradient Bandit algorithm, controlled by argumets ```gradient_alpha``` and ```use_reward_baseline``` for defining the parameters $\alpha$ and the use of average rewards $\bar{R}_{t}$ as the baseline to which compare the reward $R_{t}$ at step $t$. The action $A_{t}$ is then selected with probability $\pi_{t}(a)=\frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}{e^{H_{b}(a)}}}$, where $H_{t}(a)$, the action preferences, are updated by:

\begin{cases}
{H_{t+1}(A_{t})=H_{t}(A_{t})+\alpha(R_{t}-\bar{R}_{t})(1-\pi_{t}(A_{t}))} \\
{H_{t+1}(a)=H_{t}(a)-\alpha(R_{t}-\bar{R}_{t})\pi_{t}(a)}\ \forall{a\neq A_{t}}
\end{cases}

From [Figure 11](#Figure11) it is possible to see that

#### _Code_
"""

bandit_gba = np.zeros((4, 3), dtype=int).tolist()
gba = [0.1, 0.5, 0.1, 0.5]
mean_q_star = 4.0
variance_q_star = 1.0

for i in range(len(gba)):
    if (i < 2):
        bandit_gba[i][0] = ArmedBandit(number_of_runs=number_runs, q_star_mean=mean_q_star, q_star_variance=variance_q_star,
                                       seeds_list=seeds[i], pull_all_arms_once=False, gradient_alpha=gba[i],
                                       use_reward_baseline=True)
    else:
        bandit_gba[i][0] = ArmedBandit(number_of_runs=number_runs, q_star_mean=mean_q_star, q_star_variance=variance_q_star,
                                       seeds_list=seeds[i], pull_all_arms_once=False, gradient_alpha=gba[i],
                                       use_reward_baseline=False)
    bandit_gba[i][1] = bandit_gba[i][0].play()
    bandit_gba[i][2] = bandit_gba[i][0].get_metrics(get_optimal_percent=True)

"""<a id="Figure11"></a>"""

gb_reward_data = pd.DataFrame(np.array([step_list, bandit_gba[0][1], bandit_gba[1][1], bandit_gba[2][1], bandit_gba[3][1]]).T,
                              columns=[r'Times $t$', (r'$\alpha=' + str(gba[0]) + r'\ (baseline)$'),
                                       (r'$\alpha=' + str(gba[1]) + r'\ (baseline)$'),
                                       (r'$\alpha=' + str(gba[2]) + r'\ (no\ baseline)$'),
                                       (r'$\alpha=' + str(gba[3]) + r'\ (no\ baseline)$')])
plotter_func(gb_reward_data, x_label='Number pulls ($t$)', y_label=('Average Reward (%d runs)' % (number_runs)),
             graph_title=r'Fig. 11: Gradient Bandit - Average Reward vs. Number Pulls', save_plot=False, save_directory=r"same")

"""<a id="Figure12"></a>"""

avg_regret_data_gba = pd.DataFrame(np.array([step_list, bandit_gba[0][2], bandit_gba[1][2], bandit_gba[2][2], bandit_gba[3][2]]).T,
                                   columns=[r'Times $t$', (r'$\alpha=' + str(gba[0]) + r'\ (baseline)$'),
                                            (r'$\alpha=' + str(gba[1]) + r'\ (baseline)$'),
                                            (r'$\alpha=' + str(gba[2]) + r'\ (no\ baseline)$'),
                                            (r'$\alpha=' + str(gba[3]) + r'\ (no\ baseline)$')])
plotter_func(avg_regret_data_gba, x_label='Number pulls ($t$)', y_label=(r'Average %% Optimal Action (%d runs)' % (number_runs)),
             graph_title=r'Fig. 12: Gradient Bandit - % Optimal Action vs. Number Pulls', use_y_limits=True, y_limits=(0, 100),
             save_plot=False, save_directory=r"same")

# %%


"""## QUESTION 2 - Dynamic Programming"""

"""## References"""
